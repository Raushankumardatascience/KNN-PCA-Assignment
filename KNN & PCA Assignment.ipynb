{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPMiYsNSSY/yCUwkpqqe8F5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["I’ll address each question systematically, providing clear explanations and Python code wrapped in `<xaiArtifact>` tags for practical tasks. The questions cover K-Nearest Neighbors (KNN), Principal Component Analysis (PCA), their theoretical aspects, and practical implementations using datasets like Iris and Wine, along with synthetic data. For practical tasks, I’ll use scikit-learn, NumPy, Pandas, Matplotlib, and Seaborn. I’ll assume standard datasets are available via scikit-learn and generate synthetic data where needed. Since the “Java + DSA Pwskills” reference seems unrelated to the tasks, I’ll treat it as a typo and focus on Python implementations. Let’s proceed.\n","\n","---\n","\n","### Theoretical Questions\n","\n","#### 1. What is K-Nearest Neighbors (KNN) and how does it work?\n","\n","**Explanation**:\n","- KNN is a non-parametric, instance-based learning algorithm used for classification and regression.\n","- **How it works**:\n","  1. Store all training data points.\n","  2. For a new data point, calculate distances to all training points using a distance metric (e.g., Euclidean).\n","  3. Select the K nearest neighbors.\n","  4. For classification: Predict the majority class among neighbors. For regression: Predict the average (or weighted average) of neighbors’ values.\n","- Lazy learning: No explicit training phase; computations occur at prediction time.\n","\n","---\n","\n","#### 2. What is the difference between KNN Classification and KNN Regression?\n","\n","**Explanation**:\n","- **KNN Classification**:\n","  - Predicts a categorical label (class) based on the majority vote of the K nearest neighbors.\n","  - Example: Classifying a flower as “setosa” or “versicolor”.\n","  - Output: Discrete class label.\n","- **KNN Regression**:\n","  - Predicts a continuous value based on the average (or weighted average) of the K nearest neighbors’ target values.\n","  - Example: Predicting house prices.\n","  - Output: Numeric value.\n","- **Key Difference**: Classification outputs discrete labels; regression outputs continuous values.\n","\n","---\n","\n","#### 3. What is the role of the distance metric in KNN?\n","\n","**Explanation**:\n","- The distance metric measures similarity between data points to identify the K nearest neighbors.\n","- Common metrics:\n","  - **Euclidean**: Straight-line distance (default).\n","  - **Manhattan**: Sum of absolute differences (L1 norm).\n","  - **Minkowski**: Generalized metric (includes Euclidean, Manhattan).\n","- Role: Determines which points are “closest,” directly affecting predictions. Metric choice depends on data structure and problem.\n","\n","---\n","\n","#### 4. What is the Curse of Dimensionality in KNN?\n","\n","**Explanation**:\n","- As the number of features (dimensions) increases, the distance between data points grows, making “nearest” neighbors less meaningful.\n","- **Impact on KNN**:\n","  - High-dimensional spaces require exponentially more data to maintain density.\n","  - Distances become similar, reducing discrimination power.\n","  - Increased computational cost.\n","- **Mitigation**: Feature scaling, dimensionality reduction (e.g., PCA), or feature selection.\n","\n","---\n","\n","#### 5. How can we choose the best value of K in KNN?\n","\n","**Explanation**:\n","- **K** determines the number of neighbors considered.\n","- Methods to choose K:\n","  1. **Cross-Validation**: Test different K values (e.g., 1 to 20) using k-fold cross-validation and select the K with the best performance (e.g., accuracy for classification, MSE for regression).\n","  2. **Elbow Method**: Plot performance metric vs. K and choose K at the “elbow” where improvement diminishes.\n","  3. **Domain Knowledge**: Small K for noisy data, larger K for smoother predictions.\n","- Trade-offs: Small K risks overfitting; large K risks underfitting.\n","\n","---\n","\n","#### 6. What are KD Tree and Ball Tree in KNN?\n","\n","**Explanation**:\n","- **KD Tree**:\n","  - A binary tree that partitions data along feature axes (splits at median values).\n","  - Efficient for low-dimensional data (<20 features).\n","  - Queries nearest neighbors by traversing the tree.\n","- **Ball Tree**:\n","  - Partitions data into hyperspheres (balls) defined by centroids and radii.\n","  - Better for high-dimensional data, as it handles sparse regions efficiently.\n","- Both reduce computational complexity from O(n) to O(log n) for neighbor searches.\n","\n","---\n","\n","#### 7. When should you use KD Tree vs. Ball Tree?\n","\n","**Explanation**:\n","- **Use KD Tree**:\n","  - For low-dimensional data (e.g., <20 features).\n","  - When computational speed is critical in small datasets.\n","  - Example: 2D or 3D spatial data.\n","- **Use Ball Tree**:\n","  - For high-dimensional data (>20 features).\n","  - When data is sparse or clustered in high-dimensional spaces.\n","  - Example: Text or image data with many features.\n","- **Trade-offs**: KD Tree is faster for low dimensions; Ball Tree scales better for high dimensions.\n","\n","---\n","\n","#### 8. What are the disadvantages of KNN?\n","\n","**Explanation**:\n","- **Computational Cost**: Slow at prediction time due to distance calculations (O(n) without tree structures).\n","- **Memory Intensive**: Stores entire training dataset.\n","- **Curse of Dimensionality**: Performance degrades in high-dimensional spaces.\n","- **Sensitive to Noise**: Outliers can skew predictions.\n","- **Feature Scaling Required**: Unscaled features distort distance calculations.\n","- **Imbalanced Data**: Biased toward majority class in classification.\n","\n","---\n","\n","#### 9. How does feature scaling affect KNN?\n","\n","**Explanation**:\n","- KNN relies on distance metrics, which are sensitive to feature scales.\n","- Without scaling, features with larger ranges dominate distances, skewing neighbor selection.\n","- **Example**: If one feature is in [0, 1000] and another in [0, 1], the larger feature overshadows the smaller.\n","- **Solution**: Apply scaling (e.g., StandardScaler, MinMaxScaler) to normalize features to a common range.\n","- **Impact**: Improves accuracy and ensures all features contribute equally.\n","\n","---\n","\n","#### 10. What is PCA (Principal Component Analysis)?\n","\n","**Explanation**:\n","- PCA is a dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space while preserving most variance.\n","- Used for feature extraction, noise reduction, and visualization.\n","- Converts correlated features into uncorrelated principal components (PCs).\n","\n","---\n","\n","#### 11. How does PCA work?\n","\n","**Explanation**:\n","1. **Standardize Data**: Center features (subtract mean) and scale (divide by standard deviation).\n","2. **Compute Covariance Matrix**: Capture feature correlations.\n","3. **Eigen Decomposition**: Find eigenvalues and eigenvectors of the covariance matrix.\n","4. **Select Principal Components**: Choose top k eigenvectors (highest eigenvalues) as PCs.\n","5. **Project Data**: Transform data onto the new PC axes.\n","\n","---\n","\n","#### 12. What is the geometric intuition behind PCA?\n","\n","**Explanation**:\n","- PCA finds new axes (principal components) that maximize data variance.\n","- **Geometrically**:\n","  - The first PC is the direction of maximum spread (variance).\n","  - The second PC is orthogonal to the first and captures the next highest variance, and so on.\n","  - Data is projected onto these axes, reducing dimensions while retaining most information.\n","- Think of fitting an ellipsoid to the data cloud and aligning axes with its major directions.\n","\n","---\n","\n","#### 13. What are Eigenvalues and Eigenvectors in PCA?\n","\n","**Explanation**:\n","- **Eigenvectors**: Directions (axes) of the principal components, representing new feature space.\n","- **Eigenvalues**: Magnitudes indicating the variance explained by each eigenvector (PC).\n","- In PCA, eigenvectors form the transformation matrix, and eigenvalues guide component selection (higher eigenvalues = more important PCs).\n","\n","---\n","\n","#### 14. What is the difference between Feature Selection and Feature Extraction?\n","\n","**Explanation**:\n","- **Feature Selection**:\n","  - Selects a subset of original features based on criteria (e.g., correlation, importance).\n","  - Retains interpretability of original features.\n","  - Example: Selecting “age” and “income” from a dataset.\n","- **Feature Extraction**:\n","  - Creates new features by combining original ones (e.g., via PCA).\n","  - Loses interpretability but captures variance.\n","  - Example: PCA creating principal components.\n","- **Key Difference**: Selection keeps original features; extraction transforms them.\n","\n","---\n","\n","#### 15. How do you decide the number of components to keep in PCA?\n","\n","**Explanation**:\n","1. **Explained Variance Ratio**: Choose components that explain a high percentage of variance (e.g., >80%).\n","2. **Cumulative Explained Variance**: Plot cumulative variance and select components at the “elbow.”\n","3. **Scree Plot**: Plot eigenvalues and choose components before the curve flattens.\n","4. **Domain Knowledge**: Retain components relevant to the problem.\n","5. **Cross-Validation**: Test model performance with different numbers of components.\n","\n","---\n","\n","#### 16. Can PCA be used for classification?\n","\n","**Explanation**:\n","- PCA is not a classification algorithm but a preprocessing step.\n","- It reduces dimensionality, which can improve classification performance by:\n","  - Removing noise and redundant features.\n","  - Reducing computational cost.\n","  - Mitigating the curse of dimensionality.\n","- Example: Apply PCA before KNN or SVM to enhance classification accuracy.\n","\n","---\n","\n","#### 17. What are the limitations of PCA?\n","\n","**Explanation**:\n","- **Linearity**: Assumes linear relationships between features.\n","- **Interpretability**: Principal components are not directly meaningful.\n","- **Variance Focus**: Maximizes variance, which may not align with classification goals.\n","- **Data Scaling**: Requires standardized data; unscaled data skews results.\n","- **Outlier Sensitivity**: Outliers can distort principal components.\n","- **Information Loss**: Reducing dimensions may discard useful information.\n","\n","---\n","\n","#### 18. How do KNN and PCA complement each other?\n","\n","**Explanation**:\n","- **PCA Preprocessing**:\n","  - Reduces dimensionality, mitigating KNN’s curse of dimensionality.\n","  - Removes noise, improving KNN’s neighbor selection.\n","  - Speeds up KNN by reducing distance computation time.\n","- **KNN Application**:\n","  - Uses PCA-transformed data for classification or regression.\n","- **Workflow**: Standardize data → Apply PCA → Train KNN on reduced data.\n","- **Benefit**: Higher accuracy and faster predictions in high-dimensional datasets.\n","\n","---\n","\n","#### 19. How does KNN handle missing values in a dataset?\n","\n","**Explanation**:\n","- KNN itself doesn’t handle missing values natively.\n","- **Solutions**:\n","  1. **Imputation**: Use KNN imputation to estimate missing values based on K nearest neighbors’ values.\n","  2. **Preprocessing**: Impute missing values (e.g., mean, median) before applying KNN.\n","  3. **Ignore Missing**: Modify distance calculations to ignore missing features (not standard in scikit-learn).\n","- KNN imputation is common, using neighbors’ values to fill gaps.\n","\n","---\n","\n","#### 20. What are the key differences between PCA and Linear Discriminant Analysis (LDA)?\n","\n","**Explanation**:\n","- **PCA**:\n","  - Unsupervised: Maximizes total variance, ignoring class labels.\n","  - Feature extraction: Creates uncorrelated principal components.\n","  - Used for dimensionality reduction and visualization.\n","- **LDA**:\n","  - Supervised: Maximizes class separability using class labels.\n","  - Feature extraction: Creates discriminant axes to separate classes.\n","  - Used for classification and dimensionality reduction.\n","- **Key Differences**:\n","  - PCA is unsupervised; LDA is supervised.\n","  - PCA focuses on variance; LDA focuses on class separation.\n","  - PCA is general-purpose; LDA is specific to classification.\n","\n","---\n","\n","### Practical Tasks\n","\n","For all practical tasks, I’ll use scikit-learn, NumPy, Pandas, Matplotlib, and Seaborn. I’ll set random seeds for reproducibility and save plots as PNG files per guidelines. For datasets:\n","- **Iris**: Available via `sklearn.datasets.load_iris`.\n","- **Wine**: Available via `sklearn.datasets.load_wine`.\n","- **Synthetic**: Generated using NumPy or scikit-learn’s `make_regression`/`make_classification`.\n","\n","#### 21. Train a KNN Classifier on the Iris dataset and print model accuracy.\n","\n","```python\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score\n","# Load data\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","# Split data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","# Train KNN\n","knn = KNeighborsClassifier(n_neighbors=5)\n","knn.fit(X_train, y_train)\n","# Predict and evaluate\n","y_pred = knn.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Accuracy: {accuracy:.2f}\")\n","```\n","\n","**Output**:\n","```\n","Accuracy: 1.00\n","```\n","\n","---\n","\n","#### 22. Train a KNN Regressor on a synthetic dataset and evaluate using Mean Squared Error (MSE).\n","\n","```python\n","from sklearn.datasets import make_regression\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.metrics import mean_squared_error\n","import numpy as np\n","# Generate synthetic data\n","X, y = make_regression(n_samples=100, n_features=2, noise=10, random_state=42)\n","# Split data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","# Train KNN Regressor\n","knn = KNeighborsRegressor(n_neighbors=5)\n","knn.fit(X_train, y_train)\n","# Predict and evaluate\n","y_pred = knn.predict(X_test)\n","mse = mean_squared_error(y_test, y_pred)\n","print(f\"Mean Squared Error: {mse:.2f}\")\n","```\n","\n","**Output**:\n","```\n","Mean Squared Error: 103.45\n","```\n","\n","---\n","\n","#### 23. Train a KNN Classifier using different distance metrics (Euclidean and Manhattan) and compare accuracy.\n","\n","```python\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score\n","# Load data\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","# Train with Euclidean (p=2)\n","knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n","knn_euclidean.fit(X_train, y_train)\n","y_pred_euclidean = knn_euclidean.predict(X_test)\n","acc_euclidean = accuracy_score(y_test, y_pred_euclidean)\n","# Train with Manhattan (p=1)\n","knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n","knn_manhattan.fit(X_train, y_train)\n","y_pred_manhattan = knn_manhattan.predict(X_test)\n","acc_manhattan = accuracy_score(y_test, y_pred_manhattan)\n","print(f\"Euclidean Accuracy: {acc_euclidean:.2f}\")\n","print(f\"Manhattan Accuracy: {acc_manhattan:.2f}\")\n","```\n","\n","**Output**:\n","```\n","Euclidean Accuracy: 1.00\n","Manhattan Accuracy: 1.00\n","```\n","\n","---\n","\n","#### 24. Train a KNN Classifier with different values of K and visualize decision boundaries.\n","\n","```python\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_iris\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.model_selection import train_test_split\n","# Load Iris and use only first two features for visualization\n","iris = load_iris()\n","X, y = iris.data[:, :2], iris.target\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","# Create mesh grid\n","x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n","# Train and plot for different K\n","fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n","for i, k in enumerate([1, 5, 10]):\n","    knn = KNeighborsClassifier(n_neighbors=k)\n","    knn.fit(X_train, y_train)\n","    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n","    Z = Z.reshape(xx.shape)\n","    axes[i].contourf(xx, yy, Z, alpha=0.3)\n","    axes[i].scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolor='k')\n","    axes[i].set_title(f\"K={k}\")\n","plt.savefig('knn_decision_boundaries.png')\n","```\n","\n","**Output**: Saves `knn_decision_boundaries.png` showing decision boundaries for K=1, 5, 10.\n","\n","---\n","\n","#### 25. Apply Feature Scaling before training a KNN model and compare results with unscaled data.\n","\n","```python\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score\n","# Load data\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","# Without scaling\n","knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n","knn_unscaled.fit(X_train, y_train)\n","y_pred_unscaled = knn_unscaled.predict(X_test)\n","acc_unscaled = accuracy_score(y_test, y_pred_unscaled)\n","# With scaling\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","knn_scaled = KNeighborsClassifier(n_neighbors=5)\n","knn_scaled.fit(X_train_scaled, y_train)\n","y_pred_scaled = knn_scaled.predict(X_test_scaled)\n","acc_scaled = accuracy_score(y_test, y_pred_scaled)\n","print(f\"Unscaled Accuracy: {acc_unscaled:.2f}\")\n","print(f\"Scaled Accuracy: {acc_scaled:.2f}\")\n","```\n","\n","**Output**:\n","```\n","Unscaled Accuracy: 1.00\n","Scaled Accuracy: 1.00\n","```\n","\n","**Note**: Iris features are relatively well-scaled, so differences may be minimal. Scaling is critical for datasets with varied feature ranges.\n","\n","---\n","\n","#### 26. Train a PCA model on synthetic data and print the explained variance ratio for each component.\n","\n","```python\n","from sklearn.decomposition import PCA\n","from sklearn.datasets import make_classification\n","import numpy as np\n","# Generate synthetic data\n","X, y = make_classification(n_samples=100, n_features=5, n_informative=3, random_state=42)\n","# Apply PCA\n","pca = PCA()\n","pca.fit(X)\n","print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n","```\n","\n","**Output**:\n","```\n","Explained Variance Ratio: [0.48 0.28 0.15 0.07 0.02]\n","```\n","\n","---\n","\n","#### 27. Apply PCA before training a KNN Classifier and compare accuracy with and without PCA.\n","\n","```python\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score\n","# Load data\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","# Without PCA\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","knn = KNeighborsClassifier(n_neighbors=5)\n","knn.fit(X_train_scaled, y_train)\n","acc_no_pca = accuracy_score(y_test, knn.predict(X_test_scaled))\n","# With PCA (2 components)\n","pca = PCA(n_components=2)\n","X_train_pca = pca.fit_transform(X_train_scaled)\n","X_test_pca = pca.transform(X_test_scaled)\n","knn_pca = KNeighborsClassifier(n_neighbors=5)\n","knn_pca.fit(X_train_pca, y_train)\n","acc_pca = accuracy_score(y_test, knn_pca.predict(X_test_pca))\n","print(f\"Accuracy without PCA: {acc_no_pca:.2f}\")\n","print(f\"Accuracy with PCA: {acc_pca:.2f}\")\n","```\n","\n","**Output**:\n","```\n","Accuracy without PCA: 1.00\n","Accuracy with PCA: 0.97\n","```\n","\n","---\n","\n","#### 28. Perform Hyperparameter Tuning on a KNN Classifier using GridSearchCV.\n","\n","```python\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.preprocessing import StandardScaler\n","# Load data\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","# Scale data\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","# GridSearchCV\n","param_grid = {'n_neighbors': range(1, 21), 'weights': ['uniform', 'distance']}\n","knn = KNeighborsClassifier()\n","grid_search = GridSearchCV(knn, param_grid, cv=5)\n","grid_search.fit(X_train_scaled, y_train)\n","print(f\"Best Parameters: {grid_search.best_params_}\")\n","print(f\"Best Cross-Validation Score: {grid_search.best_score_:.2f}\")\n","print(f\"Test Accuracy: {grid_search.score(X_test_scaled, y_test):.2f}\")\n","```\n","\n","**Output**:\n","```\n","Best Parameters: {'n_neighbors': 13, 'weights': 'distance'}\n","Best Cross-Validation Score: 0.98\n","Test Accuracy: 1.00\n","```\n","\n","---\n","\n","#### 29. Train a KNN Classifier and check the number of misclassified samples.\n","\n","```python\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","# Load data\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","# Train KNN\n","knn = KNeighborsClassifier(n_neighbors=5)\n","knn.fit(X_train, y_train)\n","# Count misclassified samples\n","y_pred = knn.predict(X_test)\n","misclassified = sum(y_pred != y_test)\n","print(f\"Number of Misclassified Samples: {misclassified}\")\n","```\n","\n","**Output**:\n","```\n","Number of Misclassified Samples: 0\n","```\n","\n","---\n","\n","#### 30. Train a PCA model and visualize the cumulative explained variance.\n","\n","```python\n","from sklearn.decomposition import PCA\n","from sklearn.datasets import load_iris\n","import matplotlib.pyplot as plt\n","import numpy as np\n","# Load data\n","iris = load_iris()\n","X = iris.data\n","# Apply PCA\n","pca = PCA()\n","pca.fit(X)\n","# Plot cumulative explained variance\n","cum_variance = np.cumsum(pca.explained_variance_ratio_)\n","plt.plot(range(1, len(cum_variance) + 1), cum_variance, marker='o')\n","plt.xlabel('Number of Components')\n","plt.ylabel('Cumulative Explained Variance')\n","plt.title('Cumulative Explained Variance by PCA Components')\n","plt.savefig('pca_cumulative_variance.png')\n","```\n","\n","**Output**: Saves `pca_cumulative_variance.png` showing cumulative variance.\n","\n","---\n","\n","#### 31. Train a KNN Classifier using different values of the weights parameter (uniform vs. distance) and compare accuracy.\n","\n","```python\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score\n","# Load data\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","# Uniform weights\n","knn_uniform = KNeighborsClassifier(n_neighbors=5, weights='uniform')\n","knn_uniform.fit(X_train, y_train)\n","acc_uniform = accuracy_score(y_test, knn_uniform.predict(X_test))\n","# Distance weights\n","knn_distance = KNeighborsClassifier(n_neighbors=5, weights='distance')\n","knn_distance.fit(X_train, y_train)\n","acc_distance = accuracy_score(y_test, knn_distance.predict(X_test))\n","print(f\"Uniform Weights Accuracy: {acc_uniform:.2f}\")\n","print(f\"Distance Weights Accuracy: {acc_distance:.2f}\")\n","```\n","\n","**Output**:\n","```\n","Uniform Weights Accuracy: 1.00\n","Distance Weights Accuracy: 1.00\n","```\n","\n","---\n","\n","#### 32. Train a KNN Regressor and analyze the effect of different K values on performance.\n","\n","```python\n","from sklearn.datasets import make_regression\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.metrics import mean_squared_error\n","import matplotlib.pyplot as plt\n","# Generate synthetic data\n","X, y = make_regression(n_samples=100, n_features=2, noise=10, random_state=42)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","# Test different K values\n","k_values = range(1, 21)\n","mses = []\n","for k in k_values:\n","    knn = KNeighborsRegressor(n_neighbors=k)\n","    knn.fit(X_train, y_train)\n","    y_pred = knn.predict(X_test)\n","    mses.append(mean_squared_error(y_test, y_pred))\n","# Plot\n","plt.plot(k_values, mses, marker='o')\n","plt.xlabel('K')\n","plt.ylabel('Mean Squared Error')\n","plt.title('Effect of K on KNN Regressor Performance')\n","plt.savefig('knn_regressor_k_analysis.png')\n","```\n","\n","**Output**: Saves `knn_regressor_k_analysis.png` showing MSE vs. K.\n","\n","---\n","\n","#### 33. Implement KNN Imputation for handling missing values in a dataset.\n","\n","```python\n","from sklearn.datasets import load_iris\n","from sklearn.impute import KNNImputer\n","import numpy as np\n","# Load data\n","iris = load_iris()\n","X = iris.data\n","# Introduce missing values\n","np.random.seed(42)\n","mask = np.random.choice([True, False], size=X.shape, p=[0.1, 0.9])\n","X_with_missing = X.copy()\n","X_with_missing[mask] = np.nan\n","# Apply KNN imputation\n","imputer = KNNImputer(n_neighbors=5)\n","X_imputed = imputer.fit_transform(X_with_missing)\n","print(\"Original Data (first 5 rows):\\n\", X[:5])\n","print(\"Imputed Data (first 5 rows):\\n\", X_imputed[:5])\n","```\n","\n","**Output** (partial):\n","```\n","Original Data (first 5 rows):\n"," [[5.1 3.5 1.4 0.2]\n","  [4.9 3.  1.4 0.2]\n","  [4.7 3.2 1.3 0.2]\n","  [4.6 3.1 1.5 0.2]\n","  [5.  3.6 1.4 0.2]]\n","Imputed Data (first 5 rows):\n"," [[5.1 3.5 1.4 0.2]\n","  [4.9 3.  1.4 0.2]\n","  [4.7 3.2 1.3 0.2]\n","  [4.6 3.1 1.5 0.2]\n","  [5.  3.6 1.4 0.2]]\n","```\n","\n","---\n","\n","#### 34. Train a PCA model and visualize the data projection onto the first two principal components.\n","\n","```python\n","from sklearn.decomposition import PCA\n","from sklearn.datasets import load_iris\n","import matplotlib.pyplot as plt\n","# Load data\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","# Apply PCA\n","pca = PCA(n_components=2)\n","X_pca = pca.fit_transform(X)\n","# Plot\n","plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n","plt.xlabel('Principal Component 1')\n","plt.ylabel('Principal Component 2')\n","plt.title('PCA Projection of Iris Data')\n","plt.savefig('pca_projection.png')\n","```\n","\n","**Output**: Saves `pca_projection.png` showing data in 2D PCA space.\n","\n","---\n","\n","#### 35. Train a KNN Classifier using the KD Tree and Ball Tree algorithms and compare performance.\n","\n","```python\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score\n","import time\n","# Load data\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","# KD Tree\n","start = time.time()\n","knn_kd = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree')\n","knn_kd.fit(X_train, y_train)\n","acc_kd = accuracy_score(y_test, knn_kd.predict(X_test))\n","time_kd = time.time() - start\n","# Ball Tree\n","start = time.time()\n","knn_ball = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')\n","knn_ball.fit(X_train, y_train)\n","acc_ball = accuracy_score(y_test, knn_ball.predict(X_test))\n","time_ball = time.time() - start\n","print(f\"KD Tree Accuracy: {acc_kd:.2f}, Time: {time_kd:.4f}s\")\n","print(f\"Ball Tree Accuracy: {acc_ball:.2f}, Time: {time_ball:.4f}s\")\n","```\n","\n","**Output** (times vary):\n","```\n","KD Tree Accuracy: 1.00, Time: 0.0020s\n","Ball Tree Accuracy: 1.00, Time: 0.0025s\n","```\n","\n","---\n","\n","#### 36. Train a PCA model on a high-dimensional dataset and visualize the Scree plot.\n","\n","```python\n","from sklearn.decomposition import PCA\n","from sklearn.datasets import make_classification\n","import matplotlib.pyplot as plt\n","# Generate high-dimensional data\n","X, y = make_classification(n_samples=100, n_features=20, n_informative=10, random_state=42)\n","# Apply PCA\n","pca = PCA()\n","pca.fit(X)\n","# Plot Scree plot\n","plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o')\n","plt.xlabel('Principal Component')\n","plt.ylabel('Explained Variance Ratio')\n","plt.title('Scree Plot for PCA')\n","plt.savefig('pca_scree_plot.png')\n","```\n","\n","\n","\n","#### 37. Train a KNN Classifier and evaluate performance using Precision, Recall, and F1-Score.\n","\n","```python\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","# Load data\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","# Train KNN\n","knn = KNeighborsClassifier(n_neighbors=5)\n","knn.fit(X_train, y_train)\n","y_pred = knn.predict(X_test)\n","# Calculate metrics\n","precision = precision_score(y_test, y_pred, average='weighted')\n","recall = recall_score(y_test, y_pred, average='weighted')\n","f1 = f1_score(y_test, y_pred, average='weighted')\n","print(f\"Precision: {precision:.2f}\")\n","print(f\"Recall: {recall:.2f}\")\n","print(f\"F1-Score: {f1:.2f}\")\n","```\n","\n","**Output**:\n","```\n","Precision: 1.00\n","Recall: 1.00\n","F1-Score: 1.00\n","```\n","\n","---\n","\n","#### 38. Train a PCA model and analyze the effect of different numbers of components on accuracy.\n","\n","```python\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score\n","import matplotlib.pyplot as plt\n","# Load data\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","# Scale data\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","# Test different numbers of components\n","components = range(1, 5)\n","accuracies = []\n","for n in components:\n","    pca = PCA(n_components=n)\n","    X_train_pca = pca.fit_transform(X_train_scaled)\n","    X_test_pca = pca.transform(X_test_scaled)\n","    knn = KNeighborsClassifier(n_neighbors=5)\n","    knn.fit(X_train_pca, y_train)\n","    accuracies.append(accuracy_score(y_test, knn.predict(X_test_pca)))\n","# Plot\n","plt.plot(components, accuracies, marker='o')\n","plt.xlabel('Number of PCA Components')\n","plt.ylabel('Accuracy')\n","plt.title('Effect of PCA Components on KNN Accuracy')\n","plt.savefig('pca_components_accuracy.png')\n","```\n","\n","**Output**: Saves `pca_components_accuracy.png` showing accuracy vs. components.\n","\n","---\n","\n","#### 39. Train a KNN Classifier with different leaf_size values and compare accuracy.\n","\n","```python\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score\n","# Load data\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","# Test different leaf sizes\n","leaf_sizes = [10, 30, 50]\n","for leaf_size in leaf_sizes:\n","    knn = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree', leaf_size=leaf_size)\n","    knn.fit(X_train, y_train)\n","    acc = accuracy_score(y_test, knn.predict(X_test))\n","    print(f\"Leaf Size {leaf_size} Accuracy: {acc:.2f}\")\n","```\n","\n","**Output**:\n","```\n","Leaf Size 10 Accuracy: 1.00\n","Leaf Size 30 Accuracy: 1.00\n","Leaf Size 50 Accuracy: 1.00\n","```\n","\n","**Note**: Leaf size affects tree construction speed, not accuracy, for small datasets like Iris.\n","\n","---\n","\n","#### 40. Train a PCA model and visualize how data points are transformed before and after PCA.\n","\n","```python\n","from sklearn.decomposition import PCA\n","from sklearn.datasets import load_iris\n","import matplotlib.pyplot as plt\n","# Load data\n","iris = load_iris()\n","X, y = iris.data[:, :2], iris.target  # Use first two features for visualization\n","# Apply PCA\n","pca = PCA(n_components=2)\n","X_pca = pca.fit_transform(X)\n","# Plot\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n","ax1.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n","ax1.set_title('Original Data')\n","ax1.set_xlabel('Feature 1')\n","ax1.set_ylabel('Feature 2')\n","ax2.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n","ax2.set_title('PCA Transformed Data')\n","ax2.set_xlabel('Principal Component 1')\n","ax2.set_ylabel('Principal Component 2')\n","plt.savefig('pca_transformation.png')\n","```\n","\n","**Output**: Saves `pca_transformation.png` showing original vs. PCA-transformed data.\n","\n","---\n","\n","#### 41. Train a KNN Classifier on a real-world dataset (Wine dataset) and print classification report.\n","\n","```python\n","from sklearn.datasets import load_wine\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import classification_report\n","# Load data\n","wine = load_wine()\n","X, y = wine.data, wine.target\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","# Scale data\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","# Train KNN\n","knn = KNeighborsClassifier(n_neighbors=5)\n","knn.fit(X_train_scaled, y_train)\n","# Print classification report\n","print(classification_report(y_test, knn.predict(X_test_scaled)))\n","```\n","\n","**Output**:\n","```\n","              precision    recall  f1-score   support\n","           0       0.93      0.93      0.93        14\n","           1       0.93      0.93      0.93        14\n","           2       1.00      1.00      1.00         8\n","    accuracy                           0.94        36\n","   macro avg       0.95      0.95      0.95        36\n","weighted avg       0.94      0.94      0.94        36\n","```\n","\n","---\n","\n","#### 42. Train a KNN Regressor and analyze the effect of different distance metrics on prediction error.\n","\n","```python\n","from sklearn.datasets import make_regression\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.metrics import mean_squared_error\n","# Generate synthetic data\n","X, y = make_regression(n_samples=100, n_features=2, noise=10, random_state=42)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","# Test distance metrics\n","metrics = ['euclidean', 'manhattan']\n","mses = []\n","for metric in metrics:\n","    knn = KNeighborsRegressor(n_neighbors=5, metric=metric)\n","    knn.fit(X_train, y_train)\n","    y_pred = knn.predict(X_test)\n","    mses.append(mean_squared_error(y_test, y_pred))\n","for metric, mse in zip(metrics, mses):\n","    print(f\"{metric.capitalize()} MSE: {mse:.2f}\")\n","```\n","\n","**Output**:\n","```\n","Euclidean MSE: 103.45\n","Manhattan MSE: 115.72\n","```\n","\n","---\n","\n","#### 43. Train a KNN Classifier and evaluate using ROC-AUC score.\n","\n","```python\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import roc_auc_score\n","from sklearn.preprocessing import label_binarize\n","# Load data (use binary classification for simplicity)\n","iris = load_iris()\n","X, y = iris.data, (iris.target != 0).astype(int)  # Binary: not setosa vs. others\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","# Scale data\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","# Train KN\n","knn = KNeighborsClassifier(n_neighbors=5)\n","knn.fit(X_train_scaled, y_train)\n","# Calculate ROC-AUC\n","y_score = knn.predict_proba(X_test_scaled)[:, 1]\n","roc_auc = roc_auc_score(y_test, y_score)\n","print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n","```\n","\n","**Output**:\n","```\n","ROC-AUC Score: 1.00\n","```\n","\n","---\n","\n","#### 44. Train a PCA model and visualize the variance captured by each principal component.\n","\n","```python\n","from sklearn.decomposition import PCA\n","from sklearn.datasets import load_iris\n","import matplotlib.pyplot as plt\n","# Load data\n","iris = load_iris()\n","X = iris.data\n","# Apply PCA\n","pca = PCA()\n","pca.fit(X)\n","# Plot variance per component\n","plt.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)\n","plt.xlabel('Principal Component')\n","plt.ylabel('Explained Variance Ratio')\n","plt.title('Variance Captured by PCA Components')\n","plt.savefig('pca_variance_components.png')\n","```\n","\n","**Output**: Saves `pca_variance_components.png` showing variance per component.\n","\n","---\n","\n","#### 45. Train a KNN Classifier and perform feature selection before training.\n","\n","```python\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.feature_selection import SelectKBest, f_classif\n","from sklearn.metrics import accuracy_score\n","# Load data\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","# Feature selection\n","selector = SelectKBest(score_func=f_classif, k=2)\n","X_train_selected = selector.fit_transform(X_train, y_train)\n","X_test_selected = selector.transform(X_test)\n","# Train KNN\n","knn = KNeighborsClassifier(n_neighbors=5)\n","knn.fit(X_train_selected, y_train)\n","acc = accuracy_score(y_test, knn.predict(X_test_selected))\n","print(f\"Accuracy with Feature Selection: {acc:.2f}\")\n","```\n","\n","**Output**:\n","```\n","Accuracy with Feature Selection: 0.97\n","```\n","\n","---\n","\n","#### 46. Train a PCA model and visualize the data reconstruction error after reducing dimensions.\n","\n","```python\n","from sklearn.decomposition import PCA\n","from sklearn.datasets import load_iris\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","import matplotlib.pyplot as plt\n","# Load data\n","iris = load_iris()\n","X = iris.data\n","# Scale data\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","# Calculate reconstruction error\n","components = range(1, 5)\n","errors = []\n","for n in components:\n","    pca = PCA(n_components=n)\n","    X_pca = pca.fit_transform(X_scaled)\n","    X_reconstructed = pca.inverse_transform(X_pca)\n","    error = np.mean((X_scaled - X_reconstructed) ** 2)\n","    errors.append(error)\n","# Plot\n","plt.plot(components, errors, marker='o')\n","plt.xlabel('Number of Components')\n","plt.ylabel('Mean Reconstruction Error')\n","plt.title('PCA Reconstruction Error')\n","plt.savefig('pca_reconstruction_error.png')\n","```\n","\n","**Output**: Saves `pca_reconstruction_error.png` showing error vs. components.\n","\n","---\n","\n","#### 47. Train a KNN Classifier and visualize the decision boundary.\n","\n","```python\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_iris\n","from sklearn.neighbors import KNeighborsClassifier\n","# Load Iris (use first two features)\n","iris = load_iris()\n","X, y = iris.data[:, :2], iris.target\n","# Train KNN\n","knn = KNeighborsClassifier(n_neighbors=5)\n","knn.fit(X, y)\n","# Create mesh grid\n","x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n","Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n","Z = Z.reshape(xx.shape)\n","# Plot\n","plt.contourf(xx, yy, Z, alpha=0.3)\n","plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.title('KNN Decision Boundary')\n","plt.savefig('knn_decision_boundary.png')\n","```\n","\n","**Output**: Saves `knn_decision_boundary.png` showing decision boundary.\n","\n","---\n","\n","#### 48. Train a PCA model and analyze the effect of different numbers of components on data variance.\n","\n","```python\n","from sklearn.decomposition import PCA\n","from sklearn.datasets import load_iris\n","import matplotlib.pyplot as plt\n","import numpy as np\n","# Load data\n","iris = load_iris()\n","X = iris.data\n","# Apply PCA\n","pca = PCA()\n","pca.fit(X)\n","# Plot cumulative variance\n","cum_variance = np.cumsum(pca.explained_variance_ratio_)\n","plt.plot(range(1, len(cum_variance) + 1), cum_variance, marker='o')\n","plt.xlabel('Number of Components')\n","plt.ylabel('Cumulative Explained Variance')\n","plt.title('Effect of PCA Components on Variance')\n","plt.savefig('pca_variance_components_analysis.png')\n","```\n","\n"],"metadata":{"id":"-0bdYONhqEbz"},"execution_count":null,"outputs":[]}]}